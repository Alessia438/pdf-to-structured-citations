<!doctype html>
<html lang="en">
<head>
	<title>PDF to Structured Citations</title>
	<!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<!-- Bootstrap CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
	<!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
    <style type="text/css">
	body {
		margin: 0;
		font-family: "Nunito Sans", sans-serif;
		font-size: 1rem;
		font-weight: 300;
		line-height: 1.5;
		color: #565264;
		text-align: left;
		background-color: #D1C3BD;
	}
	button.btn-link{
		color: #565264;
		text-decoration: none;
		font-size: bold;
		text-decoration:none; 
	}

	button.btn-link:hover{
		color: #735F6B; 
		text-decoration:none; 
		cursor:pointer;  
	}
	.card-body{color: black;}
	.card {
		margin: 0 auto;
		float: none;
		margin-bottom: 10px;
	}
	ul li, ol li{text-align: left;}
	a.back-to-top.dark {
		margin: auto;
		padding-left: 30vw;
	}
	p{text-align:justify;}
	h1 {
	    font-family: Snell Roundhand;
	    text-align: center;
	    margin-top: 10vh;
	    font-size: xxx-large;
	}
	h2, h3{
		padding: 50px 0px 30px 0px;
	}

	h4{padding: 30px 20px 20px 0px;}

	#protocol{    
		text-align: justify;
	    border-style: solid;
	    padding: 40px 60px 40px 60px;
	}
	span.highlight{background-color: yellow;}

	pre, code {
	  font-family: monospace, monospace;
	}
	pre {
	  overflow: auto;
	}
	pre > code {
	  display: block;
	  padding: 1rem;
	  word-wrap: normal;
	}


    </style>
 
</head>
<body>
	<nav class="navbar navbar-expand-lg navbar-light bg-light">
	  <a class="navbar-brand">to_SCi</a>
	  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
	    <span class="navbar-toggler-icon"></span>
	  </button>
	  <div class="collapse navbar-collapse" id="navbarNav">
	    <ul class="navbar-nav">
	      <li class="nav-item">
	        <a class="nav-link" href="#">Home</a>
	      </li>
	      <li class="nav-item">
	        <a class="nav-link" href="https://github.com/Alessia438/pdf-to-structured-citations" target=”_blank”>GitHub_Repository</a>
	      </li>
	    </ul>
	  </div>
	</nav>

	<h1>PDF to structured citations</h1>
	
	<!-- <div id="accordion"> -->
	<div>


<!-- first card -->
<div class="card" style="width: 90vw; margin-top: 20vh;">
	    <div class="card-header" id="headingOne">
	      <h5 class="mb-0" style="text-align: center;">
	        <button class="btn btn-link" data-toggle="collapse" data-target="#collapseOne" aria-expanded="true" aria-controls="collapseOne">
	          Next meeting: Tuesday 10/12/2021
	        </button>
	      </h5>
	    </div>

	    <div id="collapseOne" class="collapse show" aria-labelledby="headingOne" data-parent="#accordion">
	      <div class="card-body" style="text-align: center;">
	        <h4>Software checking</h4>
	        <p>Up to now I could verify the validity of 4 among applications and libraries:</p>
	        <ul>
	        	<li>CERMINE,ONLINE</li>
	        	<li>EXCITE,ONLINE</li>
				<li>Scholarcy,ONLINE</li>
				<li>GROBID,DOWNLOAD</li>
	        </ul>
	        Three of these four were online and I simply had to verify whether they work correctly. Instead, I downloaded Grobid. It was quite easy to understand and to manage since there is a good documentation. Differently, some other software I tried to download but which I couldn't manage completely have several problems including lack of documentation (for example for the java library Dr Inventor) and versioning problems (for instance pdffsa4met was lastly committed in 2013 and the software is not working with python3).
	        <br>
	        
	        <h4>Gold standard creation</h4>
	        <p>For what concerns the gold standard I have made progresses on three aspects:</p>
	        <ol>
	        	<li>METADATA: I have selected the final set of minimum metadata to include in the gold standard for the referneces extraction.</li>
	        	<li>FORMAT: at the moment the format which seems the most suitable to be used as reference model is XML. Indeed, all the tools selected up to now have as main or optional output format XMl, even with different standard. Grobid makes use of TEI as standard model for the XML encoding and it would be the best choice since it is well documented. But before choosing the final format I would prefer to be sure about all the remaining output formats.</li>
	        	<li>REFERENCES: I started to extract the references from the papers. This is a process which I want to carry on in parallel with the software installation so that when it will be chosen a format I will have the references ready to be encoded. </li>
	        </ol>
	        <p>The next step, after the selection of the final format, will be the creation of the conversion algorithms between different file formats.</p>
			    
	      	</div>
	    </div>
	  </div>
<!-- end of first card -->


<!-- first card -->
<div class="card" style="width: 90vw;">
	    <div class="card-header" id="headingOne">
	      <h5 class="mb-0" style="text-align: center;">
	        <button class="btn btn-link" data-toggle="collapse" data-target="#collapseOne" aria-expanded="true" aria-controls="collapseOne">
	          Tuesday 10/05/2021
	        </button>
	      </h5>
	    </div>

	    <div id="collapseOne" class="collapse" aria-labelledby="headingOne" data-parent="#accordion">
	      <div class="card-body" style="text-align: center;">
	        <h4>Software checking</h4>
	        <p>I have made a final check of the software retrieved up to know. From 21 retrieved fromt he literature review:</p>
	        <ol>
	        	<li>8 are invalid (because they are not still available or it is not provided a link useful to retrieve them);</li>
	        	<li>13 are valid (some of them are still to download so this number is variable on the basis of the actual usability);</li>
	        </ol>
	        <p>Up to now the valid software retrived are:</p>
	        <ul>
	        	<li>CERMINE,ONLINE</li>
	        	<li>EXCITE,ONLINE</li>
				<li>Scholarcy,ONLINE</li>
				<li>cb2Bib,DOWNLOAD</li>
				<li>Dr. Inventor,DOWNLOAD</li>
				<li>GROBID,DOWNLOAD</li>
				<li>OCR++,DOWNLOAD</li>
				<li>pdfssa4met,DOWNLOAD</li>
				<li>PDFMEF,DOWNLOAD</li>
				<li>Science Parse,DOWNLOAD</li>
				<li>PDFX,DOWNLOAD</li>
				<li>ParsCit,DOWNLOAD</li>
				<li>PdfAct (ex. IceCite),?</li>
	        </ul>
	        <h5 style="color: grey; margin-top: 10vh;">Doubts</h5>
			<ul style="list-style-type: none;">
				<li>
					The question mark next to the last voice in the software list refers to a doubt about IceCite/PdfAct software. Indeed, in the referring paper (<a href="https://rd.springer.com/chapter/10.1007/978-3-642-41154-0_30">The Icecite Research Paper Management System</a>), it is said that IceCite is able to extract references from PDF papers, but on the <a href="https://github.com/ckorzen/icecite">IceCite GitHub repository</a> it is written that there is a new software to use in order to carry out the text extraction task, PdfAct. At the same time, on the <a href="https://github.com/ad-freiburg/pdfact">PdfAct GitHub repo</a>, it is written that PdfAct is able to carry out only a small set of extraction, mainly based on text blocks, and there is no further spcification about references extraction. Also, there could be the <a href="http://www.icecite.org/">IceCite</a> online tool, but it requires an authorization to access it (it returns error 401). Actually I have written to the creator of both the repositories in order to understand if one of the two software can be used in order to succede in the reference extraction task or, in case, if I can get access to the online tool.
				</li>
			</ul>
	        
	        <h4>Gold standard creation</h4>
	        <p>The first of the tasks necessary for the creation of the gold standard I have taken into consideration is the selection of the metadata. At the moment I have considered 5 kind of publications: article, book, book chapter, proceeding and preprint. I have considered in all the cases the average of the reported metadata, but in some cases I have considered also some metadata which were not in all the kinds of publication but that were represented by similar categories in all of them. For instance, in the preprints some kinds of publication have the DOI, some others the URL, some other both of them. But since they all represent an identification of the online resource I have considered them as an alternative (DOI &#8744; URL). The remaining publication types will be updated by the next week.</p>

	        <h5 style="color: grey; margin-top: 10vh;">Doubts</h5>
			<ul style="list-style-type: none;">
				<li>
					For what I have seen, the papers I should consider to create the gold standard are included between these 5-10 types. Am I supposed to consider all the possible options, just like in the source table, or is it fine if I consider only the kind of publications which will be effectively used (e.g. I don't think I will have to deal with web pages or unpublished works or online databases)?
				</li>
			</ul>
			    
	      	</div>
	    </div>
	  </div>
<!-- end of first card -->


<!-- first card -->
<div class="card" style="width: 90vw;">
	    <div class="card-header" id="headingOne">
	      <h5 class="mb-0" style="text-align: center;">
	        <button class="btn btn-link" data-toggle="collapse" data-target="#collapseOne" aria-expanded="true" aria-controls="collapseOne">
	          Tuesday 09/28/2021
	        </button>
	      </h5>
	    </div>

	    <div id="collapseOne" class="collapse" aria-labelledby="headingOne" data-parent="#accordion">
	      <div class="card-body" style="text-align: center;">
	        <h4>Literature review</h4>
	        <p>The final numbers of the papers accepted/discarded is changed because of the full-text reading:</p>
	        <ul>
	        	<li><b>1298 papers</b> rejected, listed in papers_to_discard.csv;</li>
	        	<li><b>35 papers</b> accepted in papers_to_keeep.csv. The number has drastically decreased from the end of the research since I considered some papers waiting to read their full-text in order to decide whether to accept them or not. Many of these were about only parsers (i.e. software not able to extract information from PDF) or PDF extractors not able to recognize and label the sections or, finally, frameworks which did not use software but rather algorithms and which are not implemented in an accessible way online. Of these 35 papers accepted:</li>
		        	<ul>
		        		<li>6 are links to softwares which were cited like this in the citing articles.</li>
		        		<li>29 are papers about single or multiple software, either valid or invalid.</li>
		        	</ul>
	        	<li><b>29 papers</b> described in the csv "papers".</li>
	        	<li><b>22 software</b> identified and reported in the csv "software".</li>
	        </ul>
	        <p>I had not enough time to verify whether all the retrieved software are available or not. Thus, when all the software will be analysed and I'll be sure about which of them are valid, and which are not, I will publish the CSV files in a folder on this GitHub repo.</p>
	        <p>
			<p>
				There are a few software which have not an available link to a repository or a website of the resource itself (either because it is not provided or because it is not working). I considered them too since there is a description about what they consist of and how they work in some papers. In order to deal with the missing links, I considered the software as invalid and I have put in the "repository" cell "N.A." (non available).
			</p>
	        <p>
	        	Finally, I have corrected the mismatching between the method I have actually followed in these months and the one described in the protocol (the search with keywords has been highlighted as optional and I have added the column of the repository link in the file "Software").
	        </p>

	        <h3>Doubts</h3>
			<ul style="list-style-type: none;">
				<li>
					I have considered a pair of software able to work with scanned documents and not or not only with digital ones because they should be considered in the related works section. Shall I remove them from the software list or is it correct to consider and evaluate them too?
				</li>
				<li style="padding-top: 25px;">
					I am not sure about the RefUTU framework (DOI:10.1145/2812428.2812469). Indeed, it seems like there is an online tool (<a href="http://soft.utu.fi/refutu/">http://soft.utu.fi/refutu/</a>) but the link takes to another web page on which there is no trace of REFUTU. Instead if I consider it for its steps, in order to implement it locally, there are some issues, in my opinion. The framework, basically, is composed by three steps: 
					<ol>
						<li>
							PREPROCESSING (optional): use a software which transforms the <b>PDF to plain text</b> (with pdfextract software);
						</li>
						<li>
							1ST STEP of RefUTU: data in plain text are <b>parsed</b> with another software (FreeCite) and the output is an XML file;
						</li>
						<li>
							2ND STEP of RefUTU: takes the output of the first step and transforms the <b>separated fields into a citation</b> (Bibtex format).
						</li>
					</ol>

					Actually I would be interested only in the first two steps but, there are two problems in my opinion: 1. I would be interested not in the RefUTU itself, but in the preprocessing and in its first step, so can I say that I am using refUTU?; 2. it may be a problem if I say that I use a software to transform the text to plain text and another to parse the data since there are plenty of parsers so, why should I not try all the parsers in this way?
				</li>
				<li style="padding-top: 25px;">
					Some typologies of software are able to extract information from PDf files and identify and label all the text blocks, including the "References" one (I have not considered the ones which are not able to identify the text blocks, e.g. <a href="https://github.com/pdfminer/pdfminer.six">pdfminer</a> or the ones which do not recognise the "References" block, like <a href="https://github.com/oyvindberg/PDFExtract/">PDFExtract</a>). For what I have seen a part of this kind of software is able to further investigate and separate the instances inside the blocks but some others are not. Thus, I was thinking about not considering the ones belonging to this second case, not because they are not precise but because I already know they wouldn't be able to identify the single references. Shall I consider them too in the related works section or can they be considered as out of topic? (e.g. "<a href="https://github.com/BMKEG/lapdftext">LA-PdfText</a>, a tool that focuses on PDF files of scientific articles and extracts LTBs based on (user-defined) rules, which must be defined for each different article layout").
				</li>
			</ul>

			<h3>Bibliography</h3>
			    <ul>
			    	<li><a href="https://ad-publications.cs.uni-freiburg.de/benchmark.pdf">A Benchmark and Evaluation for Text Extraction from PDF</a></li>
			    	<li><a href="https://scfbm.biomedcentral.com/articles/10.1186/1751-0473-7-7">Layout-aware text extraction from full-text PDF of scientific articles</a></li>
			    	<li><a href="https://github.com/pdfminer/pdfminer.six">pdfminer</a></li>
			    	<li><a href="https://doi.org/10.1145/2812428.2812469">RefUTU–Automatic Bibliography Database Generation for Freely Formatted Reference Listings</a></li>
			    	<li><a href="https://github.com/oyvindberg/PDFExtract/">PDFExtract</a></li>
			    </ul>
			    
	      	</div>
	    </div>
	  </div>
<!-- end of first card -->


<!-- first card -->
<div class="card" style="width: 90vw;">
	    <div class="card-header" id="headingTwo">
	      <h5 class="mb-0" style="text-align: center;">
	        <button class="btn btn-link" data-toggle="collapse" data-target="#collapseTwo" aria-expanded="true" aria-controls="collapseTwo">
	          Tuesday 09/21/2021
	        </button>
	      </h5>
	    </div>

	    <div id="collapseTwo" class="collapse" aria-labelledby="headingTwo" data-parent="#accordion">
	      <div class="card-body" style="text-align: center;">
	        <h4>Literature review</h4>
	        <p>The literature review is finished with the following numbers:</p>
	        <ul>
	        	<li><b>69 papers</b> accepted in papers_to_keeep.csv. Of these 69 papers, 12 are links to softwares which were cited like this in the citing articles.</li>
	        	<li><b>1264 papers</b> rejected, listed in papers_to_discard.csv;</li>
	        	<li><b>0 papers</b> waiting in papers_to_analyse.</li>
	        </ul>
	        <p>
	        	Following the protocol instructions, I am looking for the full text of the papers. Some of the ones accepted for the abstract have been discarded by looking at the full text.
	        	Contemporaneously I have created the two files reported as the last step of the protocol: "<i>papers</i>", where the accepted papers are reported in association with the software they describe, and "<i>software</i>" where the software are listed together with their repository/interface link and their validity status.
	        	For each paper accepted because its full text is in line with the standards of this research, it is reported in "papers", and if the software is not already present in "software", it is added there too.
	        	This step is completely manual but I hope that by working parallely on these three (four counting also papers to discard) files, the work will be a little faster.
	        </p>

	        <h3>Doubts</h3>
			<ul style="list-style-type: none;">
				<li>
					For what regards the keyword search, it seems like there are plenty of papers which I have not yet considered in this research and which come out with the keywords selected in the protocol (even when searching for combinations of many relevant keywords and not just when looking for single keywords). At the same time it seems strange to me that ther are other existing (and relevant) software which I have to consider since up to now I have retrieved among 15 and 20+ software/ pipelines/ frameworks.
					It is probable that the effort for analysing other papers would require some other weeks with the risk of not obtaining relevant results. Therefore I am not sure about what to do in this case.
				</li>
			</ul>
			    
	      	</div>
	    </div>
	  </div>
<!-- end of first card -->


<!-- card two-->
<div class="card" style="width: 90vw;">
	    <div class="card-header" id="headingTwo">
	      <h5 class="mb-0" style="text-align: center;">
	        <button class="btn btn-link" data-toggle="collapse" data-target="#collapseTwo" aria-expanded="true" aria-controls="collapseTwo">
	          Tuesday 09/14/2021
	        </button>
	      </h5>
	    </div>

	    <div id="collapseTwo" class="collapse" aria-labelledby="headingTwo" data-parent="#accordion">
	      <div class="card-body" style="text-align: center;">
	        <h4>Literature review</h4>
	        <p>The literature review is proceeding with the following numbers:</p>
	        <ul>
	        	<li><b>62 papers</b> accepted in papers_to_keeep.csv. These 62 are not all papers: 9 of them are links to software retrieved among the references of some of the other accepted papers.</li>
	        	<li><b>1135 papers</b> rejected, listed in papers_to_discard.csv;</li>
	        	<li><b>60 papers</b> waiting in papers_to_analyse.</li>
	        </ul>
	        <p>
	        	
	        </p>
			    
	      	</div>
	    </div>
	  </div>
<!-- end of card two -->


<!-- card three-->
<div class="card" style="width: 90vw;">
	    <div class="card-header" id="headingThree">
	      <h5 class="mb-0" style="text-align: center;">
	        <button class="btn btn-link" data-toggle="collapse" data-target="#collapseThree" aria-expanded="true" aria-controls="collapseThree">
	          Tuesday 09/07/2021
	        </button>
	      </h5>
	    </div>

	    <div id="collapseThree" class="collapse" aria-labelledby="headingThree" data-parent="#accordion">
	      <div class="card-body" style="text-align: center;">
	        <h4>Literature review</h4>
	        <p>The literature review is proceeding with the following numbers:</p>
	        <ul>
	        	<li><b>38 papers</b> accepted in papers_to_keeep.csv.</li>
	        	<li><b>850 papers</b> rejected, listed in papers_to_discard.csv;</li>
	        	<li><b>153 papers</b> waiting in papers_to_analyse.</li>
	        </ul>
	        <p>
	        	Some of the remaining papers are actually well-known and widely reused since they have been cited more than 100/200 and sometimes 500 times (so for each of them I have to check 100 articles). Also, in most cases the number of references of these widespread papers is quite high too (from 30 to 50/60). Therefore, I am trying to use as much as I can the python function for retrieving the papers in Crossref and checking the respective abstract but I am spending and I'll be spending a lot of time too in manually checking the remaining abstracts.
	        </p>
			    
	      	</div>
	    </div>
	  </div>
<!-- end of card three-->


<!-- card four-->
<div class="card" style="width: 90vw;">
	    <div class="card-header" id="headingFour">
	      <h5 class="mb-0" style="text-align: center;">
	        <button class="btn btn-link" data-toggle="collapse" data-target="#collapseFour" aria-expanded="true" aria-controls="collapseFour">
	          Tuesday 08/30/2021
	        </button>
	      </h5>
	    </div>

	    <div id="collapseFour" class="collapse" aria-labelledby="headingFour" data-parent="#accordion">
	      <div class="card-body" style="text-align: center;">
	        <h4>Literature review</h4>
	        <p>The literature review is proceeding with the following numbers:</p>
	        <ul>
	        	<li><b>37 papers</b> accepted in papers_to_keeep.csv. Some of them seem really interesting and useful, I hope that a full reading will allow to have a wider view on smart solutions to the thesis issue.</li>
	        	<li><b>689 papers</b> rejected, listed in papers_to_discard.csv;</li>
	        	<li><b>240 papers</b> waiting in papers_to_analyse.</li>
	        </ul>
	        <p>
	        	I have added in the protocol a note in which there are explained the two ways through which I have tried to fastly reduce the total number of papers waiting to be analysed (the automatic check of the abstracts and the search by keywords). They have been added in the shape of a suggestion to the readers/reusers.
	        	The technique of the keywords was really useful in order to remove large quantities of non relevant papers. Now I am focusing on a research made paper by paper since it seems that most of the remaining will require a little more attention than the others in order to define whether they are relevant or not.
	        </p>
	        <h3>Doubts</h3>
			<ul style="list-style-type: none;">
				<li>
					Is there a way to put in evidence that I am interested only in some chapters of a paper/thesis I have selected in the literature review and not in the entire work?
				</li>
			</ul>
			    
	      	</div>
	    </div>
	  </div>
<!-- end of card four -->


<!-- card five-->
<div class="card" style="width: 90vw;">
	    <div class="card-header" id="headingFive">
	      <h5 class="mb-0" style="text-align: center;">
	        <button class="btn btn-link" data-toggle="collapse" data-target="#collapseFive" aria-expanded="true" aria-controls="collapseFive">
	          Tuesday 08/10/2021
	        </button>
	      </h5>
	    </div>

	    <div id="collapseFive" class="collapse" aria-labelledby="headingFive" data-parent="#accordion">
	      <div class="card-body" style="text-align: center;">
	        <h4>Literature review</h4>
	        <p>The literature review is proceeding with the following numbers:</p>
	        <ul>
	        	<li><b>33 papers</b> accepted in papers_to_keeep.csv. In these papers there are both the seed papers and newly retrieved papers carrying information about new methods and softwares for citation/bibliography/references extraction;</li>
	        	<li><b>482 papers</b> rejected for the language, date (there are many papers ranging between 1980 and 2004) or for the not effectively pertinent content with respect to the research;</li>
	        	<li><b>394 papers</b> waiting in papers_to_analyse.</li>
	        </ul>
	        <p>
	        	I am continuing the strategy started the previous week by searching for the papers whose titles include certain words related to topics not directly connected to my research topic. This week I was able to reduce of only 47 papers, but I am pretty sure that I can find oter relevant keywords in order to easily find other non relevant papers.
	        </p>
			    
	      	</div>
	    </div>
	  </div>
<!-- end of card Five -->


<!-- Six card -->
<div class="card" style="width: 90vw;">
	    <div class="card-header" id="headingSix">
	      <h5 class="mb-0" style="text-align: center;">
	        <button class="btn btn-link" data-toggle="collapse" data-target="#collapseSix" aria-expanded="true" aria-controls="collapseSix">
	          Tuesday 08/04/2021
	        </button>
	      </h5>
	    </div>

	    <div id="collapseSix" class="collapse" aria-labelledby="headingSix" data-parent="#accordion">
	      <div class="card-body" style="text-align: center;">
	        <h4>Literature review</h4>
	        <p>The literature review is proceeding with the following numbers:</p>
	        <ul>
	        	<li><b>32 papers</b> accepted in papers_to_keeep.csv. In these papers there are both the seed papers and newly retrieved papers carrying information about new methods and softwares for citation/bibliography/references extraction;</li>
	        	<li><b>400 papers</b> rejected for the language, date (there are many papers ranging between 1980 and 2004) or for the not effectively pertinent content with respect to the research;</li>
	        	<li><b>441 papers</b> waiting in papers_to_analyse. Most of these papers have been retrieved through Google Scholar, Lens and Microsoft Academic.</li>
	        </ul>

	        <p>The function I created in order to retrieve the abstract was not as helpful as I hoped. Indeed on 592 total papers, 210 were not searchable on Crossref because they were not provided with a DOI and only 50 abstract were found. On those 50 papers I proceeded with a keyword analysis and I was able to discard 15 of them. But since it seemed to me that there was not enough data in order to decide in an automatized way whether the remaining papers were admissible or not, I decided to continue reading the abstract manually as I was doing before. Nonetheless, by working on specific keywords like 'covid', 'knowledge graph', 'bibliometrics' (presumably not connected to the research topic), I was able to retrieve a discrete number of papers (154) which could be discarded on the basis of title, subtitle, keywords and of a brief reading of the abstract.</p>
			    
	      	</div>
	    </div>
	  </div>
<!-- end of card  Six-->


<!-- Seven card -->
<div class="card" style="width: 90vw;">
	    <div class="card-header" id="headingSeven">
	      <h5 class="mb-0" style="text-align: center;">
	        <button class="btn btn-link" data-toggle="collapse" data-target="#collapseSeven" aria-expanded="true" aria-controls="collapseSeven">
	          Tuesday 07/26/2021
	        </button>
	      </h5>
	    </div>

	    <div id="collapseSeven" class="collapse" aria-labelledby="headingSeven" data-parent="#accordion">
	      <div class="card-body" style="text-align: center;">
	        <h4>Literature review</h4>
	        <p>The literature review is proceeding with the following numbers:</p>
	        <ul>
	        	<li><b>30 papers</b> accepted in papers_to_keeep.csv. In these papers there are both the seed papers and newly retrieved papers carrying information about new methods and softwares for citation/bibliography/references extraction;</li>
	        	<li><b>192 papers</b> rejected for the language, date (there are many papers ranging between 1980 and 2004) or for the not effectively pertinent content with respect to the research;</li>
	        	<li><b>595 papers</b> waiting in papers_to_analyse. Most of these papers have been retrieved through Google Scholar, Lens and Microsoft Academic.</li>
	        </ul>

	        <p>In this week, the most relevant aspect of the research is that the number of newly retrieved papers is slowly decreasing while the number of rejected papers is exponencially incresing and the number of accepted papers is almost stable.</p>
			    
	      	</div>
	    </div>
	  </div>
<!-- end of card Seven -->


<!-- Eightth card -->
<div class="card" style="width: 90vw;">
	    <div class="card-header" id="headingEight">
	      <h5 class="mb-0" style="text-align: center;">
	        <button class="btn btn-link" data-toggle="collapse" data-target="#collapseEight" aria-expanded="true" aria-controls="collapseEight">
	          Tuesday 07/19/2021
	        </button>
	      </h5>
	    </div>

	    <div id="collapseEight" class="collapse" aria-labelledby="headingEight" data-parent="#accordion">
	      <div class="card-body" style="text-align: center;">
	        <h4>Literature review</h4>
	        <p>The literature review is proceeding with the following numbers:</p>
	        <ul>
	        	<li><b>21 papers</b> accepted in papers_to_keeep.csv. In these papers there are both the seed papers and newly retrieved papers carrying information about new methods and softwares for citation/bibliography/references extraction;</li>
	        	<li><b>81 papers</b> rejected for the language or, mostly, for the date (there are many papers ranging between 1980 and 2004) or for the not effectively pertinent content with respect to the research;</li>
	        	<li><b>542 papers</b> waiting in papers_to_analyse. Most of these papers have been retrieved through Google Scholar, Lens and Microsoft Academic.</li>
	        </ul>

	        <h3>Doubts</h3>
			<ul style="list-style-type: none;">
				<li>
					Some papers regard the extraction and manipulation from html files, e.g. "Locating and parsing bibliographic references in html medical articles". Can I reasonably exclude them from the literature review?
				</li>
				<li style="padding-top: 25px;">
					There are some papers available only behind payment, is it correct if I do not consider them neither?
				</li>
			</ul>
			    
	      	</div>
	    </div>
	  </div>
<!-- end of Eightth card -->


<!-- Eigth card -->
<div class="card" style="width: 90vw;">
	    <div class="card-header" id="headingNine">
	      <h5 class="mb-0" style="text-align: center;">
	        <button class="btn btn-link" data-toggle="collapse" data-target="#collapseNine" aria-expanded="true" aria-controls="collapseNine">
	          Wednesday 07/14/2021
	        </button>
	      </h5>
	    </div>

	    <div id="collapseNine" class="collapse" aria-labelledby="headingNine" data-parent="#accordion">
	      <div class="card-body" style="text-align: center;">
	        <h4>Literature review</h4>
	        <p>The literature review is proceeding with the following numbers:</p>
	        <ul>
	        	<li><b>18 papers</b> accepted in papers_to_keeep.csv. In these papers there are both the seed papers and newly retrieved papers carrying information about new methods and softwares for citation/bibliography/references extraction;</li>
	        	<li><b>51 papers</b> rejected for the language or, mostly, for the date (there are many papers ranging between 1980 and 2004) or for the not effectively pertinent content with respect to the research;</li>
	        	<li><b>494 papers</b> waiting in papers_to_analyse. Most of these papers have been retrieved through Google Scholar, Lens and Microsoft Academic. It is clear with respect to the results of the previous meeting, that the proportion between the papers accepted and the newly found ones is definetly unbalanced towards the citing/cited ones and it seems quite far the point in which this balance changes (where I can find always less new papers).</li>
	        </ul>
	        <p>The section through which I can get the papers cited by the current ones (the references) has been completed and it is revealed to be helpful in order to retrieve as many new papers in an automated way as possible.</p>
			    
	      	</div>
	    </div>
	  </div>
<!-- end of Eigth card -->


<!-- Ten card -->
<div class="card" style="width: 90vw;">
	    <div class="card-header" id="headingTen">
	      <h5 class="mb-0" style="text-align: center;">
	        <button class="btn btn-link" data-toggle="collapse" data-target="#collapseTen" aria-expanded="true" aria-controls="collapseTen">
	          Tuesday 07/06/2021
	        </button>
	      </h5>
	    </div>

	    <div id="collapseTen" class="collapse" aria-labelledby="headingTen" data-parent="#accordion">
	      <div class="card-body" style="text-align: center;">
	        <h4>Literature review</h4>
	        <p>The literature review is proceeding with the following numbers:</p>
	        <ul>
	        	<li>10 papers accepted in papers_to_keeep.csv (still seed papers);</li>
	        	<li>16 papers rejected because of the language (e.g. not english but German or Japanese) or the date (before 2005);</li>
	        	<li>240 papers waiting in papers_to_analyse. Most of these papers have been retrieved through Google Scholar, while ISI and Lens had not so much information.</li>
	        </ul>
	        <p>In the code used to search the citing papers through API requests I have added a section through which I can get the papers cited by the current ones (the references). It is not already finished but by today it should be. In this way I have the possibility to fasten up a little more this part of the literature review.</p>

			    
	      	</div>
	    </div>
	  </div>
<!-- end of Ten card -->


<!-- Eleventh card -->
<div class="card" style="width: 90vw;">
	    <div class="card-header" id="headingEleven">
	      <h5 class="mb-0" style="text-align: center;">
	        <button class="btn btn-link" data-toggle="collapse" data-target="#collapseEleven" aria-expanded="true" aria-controls="collapseEleven">
	          Friday 07/02/2021
	        </button>
	      </h5>
	    </div>

	    <div id="collapseEleven" class="collapse" aria-labelledby="headingEleven" data-parent="#accordion">
	      <div class="card-body" style="text-align: center;">
	        <h4>Literature review</h4>
	        <p>The literature review is proceeding with the following numbers:</p>
	        <ul>
	        	<li>3 papers accepted in papers_to_keeep.csv (still seed papers);</li>
	        	<li>10 papers rejected because of the language (e.g. not english but German or Japanese) or the date (before 2005);</li>
	        	<li>215 papers waiting in papers_to_analyse. Most of these papers have been retrieved through Google Scholar, while ISI and Lens had not so much information.</li>
	        </ul>
	        <p>From the previus meeting the most relevant change is the creation of a function which retrieved information from the API of COCI from OpenCitatiosn. The function is able to retrieve the DOIs of the citing papers by sending a request to the COCI API and the respective contextual information from Crossref, so that it is able to automatically create the line to insert in the csv files.</p>

			<br> 

			<h3>Doubts</h3>
			<ul style="list-style-type: none;">
				<li>
					I am using COCI API from OpenCitations, the one we also used in the Open Science course, in order to search more citational information about the already retrieved papers. It is actually useful because it contains lots of information about citing papers, at least up to now. But I was wondering if it is enough to use COCI Api or if it would be better to use other APIs from OpenCitations like CROCI or CCC rest apis in order to have a complete overciew of the citational information.
				</li>
				<li style="padding-top: 25px;">
					I had a problem with Google Scholar since, by working in two different days on the results of the same research, I discovered that the order in which I had inserted the data in the CSV on the first day was (it is not clear why) different from the one reported on the second day by Google Scholar. Is it possible that the order of the results on Google Scholar changes so rapidly that the order is different from one day to the next one? Also, this is not only a problem of order inside the CSV since, apart from this difference, which I am pretty sure is not due to my function since I always work with the data ordered and I would see if the data are inserted in a different order, it has happened that some papers I had inserted the previous day were no more in the lists of Google Scholar, and some others were not in my CSV. I am not sure about how to act if this problem continues with all the papers. 
				</li>
				<li style="padding-top: 25px;">
					I have two doubts about the OpenCitations and Crossref APIs: if they work only with DOI and I am not able to retrieve the DOI of a publication, is there, nonetheless, a way to use the API or a way to look for a publication even if the DOI is missing? Also, not exactly a doubt, I have seen that the language is a metadata not always available in the Crossref results, so that step must be done manually. Id ecided to create a new temporary file in which the function writes the new rows which then I personally check in order to verify whehter the language is english/italian or not. I know it may not be the best solution but I couldn't find another way.
				</li>
				<li style="padding-top: 25px;">
					I was looking if there exists a Google Scholar API, since it is easier to use in a mostly automated way. I've seen that there is only SERPAPI, which I am not sure is a valid option in order to retrieve all the information about citing publications.
				</li>
			</ul>
			    
	      	</div>
	    </div>
	  </div>
<!-- end of Eleventh card -->


<!-- Twelth card -->
<div class="card" style="width: 90vw;">
	    <div class="card-header" id="headingTwelve">
	      <h5 class="mb-0" style="text-align: center;">
	        <button class="btn btn-link" data-toggle="collapse" data-target="#collapseTwelve" aria-expanded="true" aria-controls="collapseTwelve">
	          Tuesday 06/15/2021
	        </button>
	      </h5>
	    </div>

	    <div id="collapseTwelve" class="collapse" aria-labelledby="headingTwelve" data-parent="#accordion">
	      <div class="card-body" style="text-align: center;">
	        <h4>Retrieving the papers for the literature review</h4>
	        <p>Starting from the first seed paper I started processing it like it is described in the protocol. Firstly, I have checked its integrity with respect to the general and more specific parameters; then, I have transferred it in papers_to_keep. Finally I proceeded with the backward and forward search. In order to do so I have retrieved a lot of papers, in particular from the backward search and from the forward search, in particular thanks to Google Scholar.</p>
	        <p>Also, since it seems like there will be lots of papers listed in all the three CSV files at a certain point (some to be fully read, some other still to be generally checked) I have tried to implement an semi-automatic way to check whether the new papers retrieved are already present in the CSV files or whether they have still to be added. Also this small software includes a part through which if the name or the ID of the paper considered ar not already listed in the CSV, they are automatically added to papers_to_analyze. But whether the data in which they have been written in smaller than 2005, then, they are added to papers_to_discard. And finally, the rest of the string necessary in order to identify the papers are added manually.</p>

	        <p>This is a piece of the code created in order to check the existence or not of the currently considered paper. It is repeated for all the three existing files:</p>
	        <div id="over_img" style="overflow: overlay;">
			  <img src="img/verify_existence.png" style="padding-top: 20px; padding-bottom: 30px">
			  </div>

	        <p>Then, this is the snippet which creates the new line in the CSV: if the paper could not be found in the CSV files, then, if the date is bigger than 2005, it is added to papers_to_keep, otherwise to papers_to_discard.</p>
			  <img src="img/add_paper.png" style="padding-top: 16px; padding-bottom: 20px">

			  <br> 

			<h3>Doubts</h3>
			<ul style="list-style-type: none;">
				<li>
					I have some doubts about how to deal with specific cases of papers: 
					<ol>
					<li>In a few cases the pages or the venue are not specified (I want to point it out because it is data which is normally present in the specifications of a paper, and it sounds strange not to retrieve that information). In these cases shall I leave a white space or there could be another explanation for this lack of data?</li>
					<li>In a particular case, it is not clear whether the number corresponds to the issue or to the volume. Nonetheless, in both the cases the other data is missing. In these "mutilated" cases should I try in any way to find out the missing data or can I simply accept the fact that it is not specified (i.e. how deep am i supposed to go in this research?)
					</li>
					<img src="img/example_issue.png" style="padding-top: 20px; padding-bottom: 30px">
					<li>Should I make clear in some way whether a book belongs to a series or books or is it enough to specify the specific book, without mentioning the collection?</li>
					</ol>
				</li>
				<li style="padding-top: 25px;"> 
					This is a more formal doubt about how to write in CSV (with the perspective of presenting this as part of the thesis material). In some fields, like the title, I need to put the double quotes in order to allow also titles with comas, which would be otherwise considered as separator among elements of the CSV. Is there a rule to decide whether to use comas or not, and eventually when?
				</li>
				<li style="padding-top: 25px;">
					I tried to semi-automate the process of verification and adding. Of course it is impossible to make it completely automatic but in this way, at leats, part of the work is done automatically. Is it fine if part of the work is carried out in this way? Shall I describe this fact in the protocol or is enough if it remains a particular implementation of my work?
				</li>
			</ul>
			    
	      	</div>
	    </div>
	  </div>
<!-- end of twelth card -->



<!-- Twelth card -->
<div class="card" style="width: 90vw;">
	    <div class="card-header" id="headingThirteen">
	      <h5 class="mb-0" style="text-align: center;">
	        <button class="btn btn-link" data-toggle="collapse" data-target="#collapseThirteen" aria-expanded="true" aria-controls="collapseThirteen">
	          Tuesday 06/10/2021
	        </button>
	      </h5>
	    </div>

	    <div id="collapseThirteen" class="collapse" aria-labelledby="headingThirteen" data-parent="#accordion">
	      <div class="card-body" style="text-align: center;">
	        <h3>Udpating the protocol for a systematic literature review</h3>
	        <p>On Protocols.io I've updated the changes required in order to make more compliant with the general requirements for an open protocol:</p>
	        <ol>
	        	<li>The final checks to the text have led to a final version of the protocol in third person and without precise references with respect to the context of the thesis.</li>
	        	
	        	<li>Also the flowchart has been updated with the corrections of the mistakes identified in the previous meeting and with a new shape, developed in one page and more functional than the previous one (by providing more space to the fourth step).</li>
	        	
	        	<li>All the suggestions provided with regards to the protocol have been applied, e.g. the platforms for the research previously missing have been added, and the ones who couldn't be addressed in this very moment have been signed in order to be provided when the materials necessary to be added/linked will be ready (i.e. the link to the protocol for the software and the URL to the software).</li>
	        </ol>
	        <p>
	        	On the other hand, since the protocol is almost finished I have started with its application:
	        	<ul>
	        		<li>I have created a folder in Zotero in which all the papers taken into consideration will be listed in order to be the basis for the bibliography of the final thesis.</li>
	        		<li>I have created the three CSV files that will be used for the research. I have selected the CSV format because it is easier to be analysed computationally even if in the protocol I have talked about generic tables. The next step will be the analysis of and the application of the research techniques to the seed papers.</li>
	        	</ul>
	        </p>
			  <br> 

			<h3>Doubts</h3>
			<ul style="list-style-type: none;">
				<li>For the research with seed papers I have pointed out one single paper which has been provided before the beginning of the research, the one about Scienceparse. Apart from this paper, other two topics have been provided, again before the research, which, instead, are two projects rather than papers (Excite and Grobid). In a first moment I decided not to add them to the seed papers list, because of this fact. But later it came out that, even if these two software (and the related documentation) cannot be defined as papers, they can be really useful in order to find out more about the panorama of software for data extraction and relative techniques. Therefore, I am not sure whether to add them to the seed papers list or not: on the one hand they are effectively "seed materials", but on the other they are not seed papers properly speking. </li>
				<li style="padding-top: 25px;">About the csv files I will have to use in the research, shall they be kept in a shared space, or it is enough if I keep them in my workspace and I make them public eventualy after the end of the research?</li>
			</ul>
			    
			<h3>Bibliography</h3>
			    <ul>
				    <li><a href="https://doi.org/10.1177/0739456X17723971" target=”_blank”>Guidance on Conducting a Systematic Literature Review</a></li>
				    <li>Kitchenham, B. A. & Charters, S. (2007). <a href="https://www.elsevier.com/__data/promis_misc/525444systematicreviewsguide.pdf" target=”_blank”>Guidelines for performing Systematic Literature Reviews</a> in Software Engineering (EBSE 2007-001). Keele University and Durham University Joint Report.</li>
				    <li><a href="https://www.aclweb.org/anthology/N18-3011.pdf" target=”_blank”>Construction of the Literature Graph in Semantic Scholar</a></li>
				    <li><a href="http://slides.com/janagelavik/grobid#/3" target=”_blank”>GROBID in 30 slides - GROBID Documentation</a></li>
				</ul>
	      	</div>
	    </div>
	  </div>
<!-- end of Twelth card -->


<!-- twelth card -->
<div class="card" style="width: 90vw;">
	    <div class="card-header" id="headingFourteen">
	      <h5 class="mb-0" style="text-align: center;">
	        <button class="btn btn-link" data-toggle="collapse" data-target="#collapseFourteen" aria-expanded="true" aria-controls="collapseFourteen">
	          Tuesday 06/01/2021
	        </button>
	      </h5>
	    </div>

	    <div id="collapseFourteen" class="collapse" aria-labelledby="headingFourteen" data-parent="#accordion">
	      <div class="card-body" style="text-align: center;">
	        <h3>Updating the protocol for a systematic literature review</h3>
	        <p>On Protocols.io I've updated the changes required in order to make more compliant with the general requirements for an open protocol:</p>
	        <ol>
	        	<li>I have updated the definitions of VALID and INVALID in the fourth section, by removing too complex criteria to verify, like the architecture and the security issues, and added more specific and easy to check ones, like verifying whether the software is free and open. Also, in the section Materials I have added th specifications of the operating systems I include in my research.</li>
	        	
	        	<li>In the table representative of the file "software_to_analyze" I have corrected the venue and added a row in which I show the example of an article.</li>
	        	<img src="img/table_new.png" style="padding-top: 20px; padding-bottom: 20px">
	        	
	        	<li>In the section Guidelines I have added a table which shortly reports the steps of the protocol, a synthetic explanation of the content of the step and respective output files. Also, I have created two different types of flowchart representing the protocol passage by passage, one developed on a single page and the ther in two different pages.</li>
	        	<img src="img/synth_table.png" style="padding-top: 20px; padding-bottom: 20px">

	        	<li>Finally, in the step 2.1, the analysis of the seed papers, I have remove the forward and backward search since it was simply a duplicate of the following step 2.2. Tehrefore, I have modified the next step so that the forward and backward search is done also on the seed papers and after on the papers retrieved through this method.</li>
	        </ol>
			  <br> 

			<h3>Doubts</h3>
			<ul style="list-style-type: none;">
				<li>About the flowchart I have added in the Guidelines section, are they acceptably clear and useful, or the table is enough in order to give a different and clearer explanation of the steps and it is not necessary to show in the detail the sequence of passages which are already explained in the main part of the protocol?</li>
			</ul>
			    
			<h3>Bibliography</h3>
			    <ul>
				    <li><a href="https://doi.org/10.1177/0739456X17723971" target=”_blank”>Guidance on Conducting a Systematic Literature Review</a></li>
				    <li>Kitchenham, B. A. & Charters, S. (2007). <a href="https://www.elsevier.com/__data/promis_misc/525444systematicreviewsguide.pdf" target=”_blank”>Guidelines for performing Systematic Literature Reviews</a> in Software Engineering (EBSE 2007-001). Keele University and Durham University Joint Report.</li>
				</ul>
	      	</div>
	    </div>
	  </div>
<!-- end of twelth card -->



<!-- fourth last card -->
<div class="card" style="width: 90vw;">
	    <div class="card-header" id="headingFifteen">
	      <h5 class="mb-0" style="text-align: center;">
	        <button class="btn btn-link" data-toggle="collapse" data-target="#collapseFifteen" aria-expanded="true" aria-controls="collapseFifteen">
	          Tuesday 05/24/2021
	        </button>
	      </h5>
	    </div>

	    <div id="collapseFifteen" class="collapse" aria-labelledby="headingFifteen" data-parent="#accordion">
	      <div class="card-body" style="text-align: center;">
	        <h3>Udpating the protocol for a systematic literature review</h3>
	        <p>On Protocols.io I've updated the changes required in order to make more compliant with the general requirements for an open protocol:</p>
	        <ol>
	        	<li>the section with the Inclusion Criteria has been removed from the first step of the protocol and moved to the materials part. Also, I have added some explanations about the reasons at the base of each of the criteria selected and I have specified that they are criteria selected by me but that they are not mandatory, and whether someone else wants to use those guidelines for other kinds of research they can simply change or substitute them. Finally, it has been implemented with the help of some images which allow to have a clearer idea about the practical consequences of the Inclusion Criteria on the selection of the texts.</li>
	        	<img src="img/IC.png" style="padding-top: 20px; padding-bottom: 20px"></a>
	        	
	        	<li>I split the research steps in three different section so that I could easily show the subparts of each step. The first one is the creation of the three files that will be used in order to classify the papers retrieved during the following steps; the second one is the research through seed papers; and the third one the research through keywords. Each of these sections has been expanded with respect to the previous version, by means of more specific indications or explanations.</li>
	        	<img src="img/subparts.png" style="padding-top: 20px; padding-bottom: 20px"></a>
	        	
	        	<li>All the input, intermediate and output files have been described in words and with a concrete example in the steps. For instance since the files we create in the first step have different contents but the same structure, I explained the different contents each of them is expected to be filled with, in words, and I draw one single table in order to show which is the structure common to all of them.</li>
	        	<img src="img/table.png" style="padding-top: 20px; padding-bottom: 20px"></a>
	        	
	        	<li>I have added more exhaustive explanations about possible unclear concepts or processes, e.g. the concept of valid/invalid in order to classify the softwares which I will take (or not) into consideration during and after the literature review.</li>
	        </ol>
			

			  <br> 

			<h3>Doubts</h3>
			<ul style="list-style-type: none;">
				<li>I have specified that the steps three and four are iterative, and, as consequence, I had to slightly modify the workflow so that if at the end of the last step of the research ("Quality assessment for full-text reading") there is the necessity of going back to search for new materials with new keywords it is possible (by going back to the previous step, "research with keywords" and strating it again with the new requried keywords). I was wondering if it makes sense to make these steps iterative or if it could be better, also from a theoretical point of view, to arrive at the "Quality assessment for full-text reading" step with all the materials already collected and, therefore, make iterative only the "research with keywords" step.</li>
				<li style="padding-top: 25px;">Does it make sense to create two lists of keywords, one for the keywords before stratting and the other for the really used keywords? It seemed useful since at the end of the research one could realize how effective were the keywords he/she has written before the research with respect to the real research. But I was wondering if it could be more reasonable to create a new list only for the new keywords (i.e. only the one not already included in the original list), or to not create two lists at all because it does not make sense with respect to the review.</li>
			</ul>
			    
			<h3>Bibliography</h3>
			    <ul>
				    <li><a href="https://doi.org/10.1177/0739456X17723971" target=”_blank”>Guidance on Conducting a Systematic Literature Review</a></li>
				    <li>Kitchenham, B. A. & Charters, S. (2007). <a href="https://www.elsevier.com/__data/promis_misc/525444systematicreviewsguide.pdf" target=”_blank”>Guidelines for performing Systematic Literature Reviews</a> in Software Engineering (EBSE 2007-001). Keele University and Durham University Joint Report.</li>
				    <li> Mark Smallcombe. <a href="https://www.xplenty.com/blog/structured-vs-unstructured-data-key-differences/#:~:text=Conclusion-,What%20is%20Structured%20Data%3F,it's%20within%20an%20RDBMS%20structure.">Structured vs Unstructured Data: 5 Key Differences.</a></li>
				    <li><a href="https://ahrefs.com/blog/ranking-number-one-is-overrated/">Ranking #1 on Google Is Overrated (Ahrefs’ Study of 100k Keywords)</a></li>
				</ul>
	      	</div>
	    </div>
	  </div>
<!-- fourth last card -->



<!-- third last card -->
<div class="card" style="width: 90vw;">
	    <div class="card-header" id="headingSixteen">
	      <h5 class="mb-0" style="text-align: center;">
	        <button class="btn btn-link" data-toggle="collapse" data-target="#collapseSixteen" aria-expanded="true" aria-controls="collapseSixteen">
	          Tuesday 05/18/2021
	        </button>
	      </h5>
	    </div>

	    <div id="collapseSixteen" class="collapse" aria-labelledby="headingSixteen" data-parent="#accordion">
	      <div class="card-body" style="text-align: center;">
	        <h3>Protocol for a systematic literature review</h3>

			<!-- begin div protocol -->

			<div id="protocol">

			<h4>Abstract</h4>
			<p>Converting unstructured data, i.e. data coded in a format which is not structured in a predefined way, such as PDF, into structured data, i.e. using clearly defined types of data organised in a structure, has several advantages. One of the most interesting consequencies is the fact that these data become easier to search, both for humans and for algorithms. Even if there are many softwares which have this objective, through a sistematic review of the existing literature it is possible to understand whether there is/are a/few software(s) whose features allow it to have better performances than the others in order to carry out a specific task in this context. 
			This protocol shows the methodology followed in order to make a sistematic review of the literature regarding the software dedivcated to the extraction and manipulation of citations from papers in PDF file format. Thus, the objective of this research, which is reflected on the flow of the literature review methodology, is that of retrieving the most suitable software for the specified purpose, i.e. <b>retrieving and manipulating citations from PDF files</b>.</p>

			<h4>Inclusion criteria:</h4>
			<p><i>General IC for papers (all of them are mandatory):</i></p>
			<ul>
				<li>Languages: en, it;</li>
				<li>Publication Date: from 2005 on.</li>
			</ul>
			<p><i>IC for screening procedure on papers (one of them is enough, but the more the better. This part should be still quite inclusive, if in doubt keep the article)</i></p>
			<ul>
				<li>Title:</li>
					<ul>
						<li>includes the name of a software for data extraction from PDF files;</li>
						<li>contains a procedure for data extraction from PDF;</li>
						<li>includes the sequence of the word ‘PDF’ and a noun/verb/adjective derived from ‘extract’.</li>
					</ul>
				<li>Abstract:</li>
					<ul>
						<li>contains the concept of “data extraction from PDF files”, also expressed in a different way (e.g. “This paper is concerned with the extrapolation of the information contained in the titles of files in PDF format.”);</li>
						<li>includes the name of a software for data extraction from PDF files.</li>
					</ul>
				<li>Keywords (attached to the article):</li>
					<ul>
						<li>a keyword expressing the concept of extraction of data from PDF file formats e.g. “PDF extraction”;</li>
						<li>two or more keywords, one including the word “PDF” and the other expressing the concept of extraction e.g. “data extraction” + “PDF” or “data extractor” + “software” + “PDF”.</li>
					</ul>
			</ul>
			<p><i>IC for the papers full text:</i></p>
			<ul>
				<li>Information about a useful and valid software (one is enough, but on the basis of the number of the required parameter confirmed the articles will have a different weight):</li>
				<ul>
					<li>The article contains a link to a useful and still valid software;</li>
					<li>The article describes a still valid software.</li>
					<li>The article contains an explicit reference to a useful and valid software;</li>
				</ul>
				<li>Information about an invalid software:</li>
				<ul>
					<li>the article contains the description or reference to a software which is not useful or no more valid.</li>
				</ul>
			</ul>


			<h4>Search strategies:</h4>
			<p><b>Useful information for the research:</b></p>
			<ol style=" list-style-type: none; padding: 0;">
				<li>Seed papers list: Construction of the Literature Graph in Semantic Scholar.</li>
				<li>Platforms for forward research: Google Scholar, the ISI Citation Index.</li>
				<li>Keywords list: “PDF extractor”, “PDF extractor software”, “extraction of information from PDF”.</li>
				<li>Free open platforms: Google Scholar, Lens, EBSCO, ProQuest, IEEE Xplore, Open citations, ACM Digital Library.</li>
			</ol>
			<p>Step 1 <i>(seed papers)</i>:</p>
			<ol>
				<li>Create two files (“Papers_to_keep”, “Papers_to_discard”), one for the papers to keep and one for the papers which are not useful with respect to the research question. The first file will contain the papers which are compliant with the General IC for papers and with at least one of the points of the IC for screening procedure on papers. These papers will be further analyzed in a second moment by reading the full text. The second file, instead, includes all the files excluded from the research because they are not compliant with at least one point of the General IC for papers or with neither of the points in the IC for screening procedure on papers. Nonetheless, this file is useful in order to keep track of the discarded papers and to check whether a newly retrieved paper appears in that list, so that we already know it must be discarded.</li>
				<li>Add the seed papers, provided by the professor as basis for the research (see ‘Seed papers list’), to the file “Papers_to_keep”.</li>
				<li>For each of the seed papers:</li>
				<ol style="list-style-type: lower-roman;">
					<li>Make backward research. Look at the articles cited by the reference papers by looking at the list of references at the end of the article. Then, for each of the articles retrieved with the backward research, if its title is not in “Papers_to_keep” or “Papers_to_discard” and it meets all the requirements listed in General IC for papers, check the title, keywords and abstract, otherwise do not take it into consideration. Whether the article seems useful for the purposes of the research (i.e. it meets at least one of the requirements listed in IC for screening procedure on papers), add its title and its DOI (or reference link if the DOI is not available) to the file “Papers_to_keep”, otherwise to “Papers_to_discard”.</li>
					<li>Make forward search. The forward research is carried out both manually and with the help of some platforms which have that specific functionality (see “Free open platforms” in the section "Materials"). Then, for each of the articles retrieved with the forward research, if its title is not in “Papers_to_keep” or “Papers_to_discard”  and it meets all the requirements listed in General IC for papers, check the title, keywords and abstract. Whether the article seems useful for the purposes of the research (i.e. it meets at least one of the requirements listed in IC for screening procedure on papers), add its title and its DOI (or reference link if the DOI is not available) to the file “Papers_to_keep”, otherwise to “Papers_to_discard”.</li>
					<li>Tick in “Papers_to_keep” the title of the paper currently taken into consideration.</li>
				</ol>
				<li>Repeat the actions listed in point 3 for all the publications retrieved with the forward and the backward research (i.e. all the publications listed in “Papers_to_keep”, excepted the ones already ticked).</li>
				<li>When no new article is retrieved by this method of research or when the operation at point 4 has been made on the last retrieved article, skip to step 2.</li>
			</ol>
			<p>Step 2 <i>(keywords)</i>:</p>
			<p>This step directly follows the first one with a complementary perspective. Indeed, it is able to fill the eventual gap of papers and publications left by the previous step. At the end of this second step, all the available documents should be retrieved.</p>
			<ol>
				<li>For each of the keywords in the “Keywords list” (in the section "Materials"): Perform a research with the selected keyword through each of the platforms listed in “Free open platforms” (in the section "Materials"). For each of the articles retrieved by each platform, if its title is not in “Papers_to_keep” or “Papers_to_discard” and it meets all the requirements listed in General IC for papers, check the title, keywords and abstract. Whether the article seems useful for the purposes of the research (i.e. it meets at least one of the requirements listed in IC for screening procedure on papers), add its title and its DOI (or reference link if the DOI is not available) to the file “Papers_to_keep”, otherwise to “Papers_to_discard”.</li>
				<li>For all the publications retrieved with the keyword search:</li>
				<ol style="list-style-type: lower-roman;">
					<li>Make backward research. Look at the articles cited by the reference papers by looking at the list of references at the end of the article. Then, for each of the articles retrieved with the backward research, if its title is not in “Papers_to_keep” or “Papers_to_discard” and it meets all the requirements listed in General IC for papers, check the title, keywords and abstract, otherwise do not take it into consideration. Whether the article seems useful for the purposes of the research (i.e. it meets at least one of the requirements listed in IC for screening procedure on papers), add its title and its DOI (or reference link if the DOI is not available) to the file “Papers_to_keep”, otherwise to “Papers_to_discard”.</li>
					<li>Make forward search. The forward research is carried out both manually and with the help of some platforms which have that specific functionality (see “Free open platforms” in the section "Materials"). Then, for each of the articles retrieved with the forward research, if its title is not in “Papers_to_keep” or “Papers_to_discard” and it meets all the requirements listed in General IC for papers, check the title, keywords and abstract. Whether the article seems useful for the purposes of the research (i.e. it meets at least one of the requirements listed in IC for screening procedure on papers), add its title and its DOI (or reference link if the DOI is not available) to the file “Papers_to_keep”, otherwise to “Papers_to_discard”.</li>
					<li>Tick in “Papers_to_keep” the title of the paper currently taken into consideration.</li>
				</ol>
				<li>Repeat the actions listed in point 2 for all the publications retrieved with the forward and the backward research (i.e. all the publications listed in “Papers_to_keep”, excepted the ones already ticked).</li>
				<li>When no new article is retrieved by this method of research or when the operation at point 4 has been made on the last retrieved article, stop the research.</li>
			</ol>

			<h4>Quality assessment</h4>
			<p>Step 1:</p>
			<p>Create 2 files (“useful_links”, “not_useful_links”). Both the files must be created on a digital file so that clusters of topics can be created. Indeed these files have a double objective:</p>
			<ol>
				<li>First of all it is used in order to separate the papers which recall a useful and valid or, vice versa, non-useful or invalid software.</li>
				<li>The second reason is that of ordering the articles on the basis of their topic (i.e. the software), so that in a following step they can by taken into consideration in groups defined by the topic.</li>
			</ol>
			<p>Step 2:</p>
			<p>For each of the papers in “Papers_to_keep” read its full text. We outline three possible cases derived from the possible outcomes of the verification whether the articles include the points listed in IC for the papers full text or not.</p>
			<ol>
				<li>If the paper does not meet any of the requirements listed in IC for the papers full text, then remove it from “Papers_to_keep” and add it to “Papers_to_discard”;</li>
				<li>Otherwise, if the paper contents meet at least one of the requirements of the first point in IC for the papers full text, add its title to “useful_software”;</li>
				<li>Otherwise, if the paper sticks to the second point in IC for the papers full text, add the paper title to the file “not_useful_software”.</li>
			</ol>

			<h4>Strategies for data extraction, synthesis, and reporting. <i>(successively)</i></h4>
			<p>NVivo (tool for data extraction and coding) (?)</p>
			</div>

			<!-- end div protocol -->

			  <br> 

			<h3>Doubts</h3>
			<ul>
				<li>Does it make sense to make a table and a flow diagram, once defined the workflow of the protocol? These would be two different ways to synthetize the flow, and help eventual re-users to better understand the process.</li>
				<li>Do I need an authomatic way to retrieve the information contained in the text, in order to speed up the retrieval process?</li>
				<li>Does it make sense to separate the information about seed papers, research tool etc. in the material section or it risks to be confusing? (And therefore it would be better to add these information in the text where they are introduced)</li>
				<li>About the iterative processes in the research section, in order to make it clear that the iterative backward and forward research has to be done on all the papers retrieved, I use the expedient of ticking each of the papers on which it has been 
					carried out the b&f research. Then, the same research has to be performed on all the non ticked remaining papers. Does it make sense or it is not really intuitive and another system would more fficient?</li>
			</ul>
			    
			<h3>Bibliography</h3>
			    <ul>
				    <li><a href="https://doi.org/10.1177/0739456X17723971" target=”_blank”>Guidance on Conducting a Systematic Literature Review</a></li>
				    <li>Kitchenham, B. A. & Charters, S. (2007). <a href="https://www.elsevier.com/__data/promis_misc/525444systematicreviewsguide.pdf" target=”_blank”>Guidelines for performing Systematic Literature Reviews</a> in Software Engineering (EBSE 2007-001). Keele University and Durham University Joint Report.</li>
				    <li> Mark Smallcombe. <a href="https://www.xplenty.com/blog/structured-vs-unstructured-data-key-differences/#:~:text=Conclusion-,What%20is%20Structured%20Data%3F,it's%20within%20an%20RDBMS%20structure.">Structured vs Unstructured Data: 5 Key Differences.</a></li>
				</ul>
	      	</div>
	    </div>
	  </div>
<!-- end of third last card -->






<!-- penultimate card -->
<div class="card" style="width: 90vw;">
	    <div class="card-header" id="headingSeventeen">
	      <h5 class="mb-0" style="text-align: center;">
	        <button class="btn btn-link" data-toggle="collapse" data-target="#collapseSeventeen" aria-expanded="true" aria-controls="collapseSeventeen">
	          Tuesday 05/11/2021
	        </button>
	      </h5>
	    </div>

	    <div id="collapseSeventeen" class="collapse" aria-labelledby="headingSeventeen" data-parent="#accordion">
	      <div class="card-body" style="text-align: center;">
	        <h3>Research</h3>
			<br>
			<h4>Features of Systematic Literature Reviews (Guidelines for performing Systematic Literature Reviews in Software Engineering)</h4>
				<p>Some of the features that differentiate a systematic review from a conventional expert literature review are:</p>
				<ol>
					<li>Systematic reviews start by defining a <span class="highlight">review protocol</span> that specifies the research question being addressed and the methods that will be used to perform the review.</li>
					<li>Systematic reviews are based on a defined <span class="highlight">search strategy</span> that aims to detect as much of the relevant literature as possible. </li>
					<li>Systematic reviews <span class="highlight">document their search strategy</span> so that readers can assess their rigour and the completeness and repeatability of the process (bearing in mind that searches of digital libraries are almost impossible to replicate). </li>
					<li>Systematic reviews require explicit <span class="highlight">inclusion and exclusion criteria</span> to assess each potential primary study.</li>
					<li>Systematic reviews specify the information to be obtained from each primary study including <span class="highlight">quality criteria</span> by which to evaluate each primary study.</li>
				</ol>
			<br>
			<h4>Structure of a systematic review (Guidance on Conducting a Systematic Literature Review)</h4>
				<p>The articles proposes a structure for the research flow which is summarized by the following list:</p>
				<ol>
					<li>planning the review:</li>
						<ul>
						<li>identify the need for a review,</li>
						<li>specify research questions,</li>
						<li>develop a review protocol.</li>
						</ul>
					<li>conducting the review:</li>
						<ul>
						<li>identify and select primary studies, </li>
						<li>extract, analyze, and synthesize data. </li>
						</ul>
					<li>reporting the review: </li>
						<ul>
						<li>write the report to disseminate their findings from the literature review.</li>
						</ul>
				</ol>
				<p>In this moment the relevant part I am interested in is "planning the review". Indeed the first step in order to carry out a sistematic review, as stated in both the previous articles, is developing a protocol with some specifications defined in the articles themselves. Then, once finished, the protocol should be validated.</p> 

		       
		<h3>First draft of the protocol for a systematic literature review</h3>

		<!-- begin div protocol -->

		<div id="protocol">

		<h4>Purpose of the study:</h4>
		<p>A systematic literature review is necessary in order to obtain information about the softwares which allow data extraction from PDF file formats. The part of the text on which this software will be applied is the citations one. Therefore the objective of this research is that of retrieving the most suitable software for that purpose. </p>

		<h4>Research questions:</h4>
		<p>Which is the software whose features are the most suitable to retrieve references textual information from PDF files and translate them into structured data?</p>

		<h4>Inclusion criteria:</h4>
		<p>IC for search strategies:</p>
		<ul>
			<li>Languages: en, it;</li>
			<li>Publication date: since 1993.</li>
		</ul>
		<p>IC for screening procedure:</p>
		<ul>
			<li>Quality criteria (still to define).</li>
		</ul>

		<h4>Search strategies:</h4>
		<p>Step 1 <i>(seed papers)</i>:</p>
		<ul>
			<li>Starting from papers and softwares proposed by the professor as basis for the research, I am making backward and forward research;</li>
			<li>PAPERS: "Construction of the Literature Graph in Semantic Scholar"</li>
			<li>PROJECTS: "Grobid", "EXCITE project".</li>
		</ul>
		<p>Step 2 <i>(keywords)</i>:</p>
		<ul>
			<li>keyword search through the most used and efficient platforms: Google Scholar, Web of Science, Lens, EBSCO, ProQuest, IEEE Xplore;</li>
			<li>forward e backward research on the papers retrieved with keywords. In particular the forward research has been carried out both manually and with the help of some platforms which have that specific functionality;</li>
			<li>KEYWORDS LIST: “PDF extractor”, “PDF extractor software”, “extraction of information from PDF”, <i>to be continued...</i></li>
			<li>PLATFORMS FOR FORWARD RESEARCH: Google Scholar, the ISI Citation Index.</li>
		</ul>
		<p>Step 3 <i>(first generic inclusion criteria)</i>:</p>
		<ul>
			<li>Keep the articles: </li>
			<ul>
				<li>retrieved with both generic and precise keywords (it allows to have a wide range of both specific resources and generic ones);</li>
				<li>after title screening (absolute first check whether the topic of the article  is effectively related to the one I am interested in).</li>
			</ul>
			<li>Merge the articles and remove duplicates. </li>
		</ul>
		<p>Step 4 <i>(stop criterion)</i>:</p>
		<ul>
			<li>Repeat steps 2 and 3 until no new articles/papers/softwares are retrieved by new searches. Then, that is the moment to stop the research.</li>
		</ul>
		</p>
		<h4>Screening procedures: <i>(successively)</i></h4>
		<h4>Quality assessment criteria <i>(successively)</i></h4>
		<h4>Strategies for data extraction, synthesis, and reporting. <i>(successively)</i></h4>
		</div>

		<!-- end div protocol -->

		  <br> 

		<h3>Doubts</h3>
		<ul>
			<li>I have a doubt about the inclusion/exclusion criteria in the protocol for the literature review: I should apply different criteria on the basis of the point of the research in which I am (e.g. during the research I should stick to generic criteria like the language of the papers or the title, while during the screening phase I should use more specific criteria like the quality of the material or the kind of data used). I am not sure about how I am supposed to manage them in a formal way. Can they be considered simply different inclusion/exclusion criteria or are they different from a formal point of view (and therefore they shall be represented in different places of the procols and in different ways)?</li>
			<li>In a prior moment I din't get the distinction between the parameters required to make a systematic literature review (the container) and the parameters required to classify the softwares (the content). Am I supposed to insert in the protocol of the research also the taxonomy for the softwares or should this second one be part of a separate file, as it is a separate concept (i.e. another protocol which reports the steps of the identification and classification of the softwares)?</li>
			<li>How should I do to formalize the protocol for the research? Would it be enough to follow the steps defined in these papers and make it available on a platform like Protocols.io?</li>
			<li>Is it necessary that I’ll submit the protocol for a peer review? In all the articles I have read there is an “evaluation” step, after the creation of the protocol, that can be carried out by different people. Am I supposed to send it to external people or the teacher's supervision is enough?</li>
		</ul>
		    
		<h3>Bibliography</h3>
		    <ul>
			    <li><a href="https://doi.org/10.1177/0739456X17723971" target=”_blank”>Guidance on Conducting a Systematic Literature Review</a></li>
			    <li>Kitchenham, B. A. & Charters, S. (2007). <a href="https://www.elsevier.com/__data/promis_misc/525444systematicreviewsguide.pdf" target=”_blank”>Guidelines for performing Systematic Literature Reviews</a> in Software Engineering (EBSE 2007-001). Keele University and Durham University Joint Report.</li>
			</ul>
	      </div>
	    </div>
	  </div>
<!-- end of penultimate card -->






<!-- last card -->
	<div class="card" style="width: 90vw;">
	    <div class="card-header" id="headingSeventeen">
	      <h5 class="mb-0" style="text-align: center;">
	        <button class="btn btn-link" data-toggle="collapse" data-target="#collapseSeventeen" aria-expanded="true" aria-controls="collapseSeventeen">
	          Tuesday 04/20/2021
	        </button>
	      </h5>
	    </div>

	    <div id="collapseSeventeen" class="collapse" aria-labelledby="headingSeventeen" data-parent="#accordion">
	      <div class="card-body" style="text-align: center;">
	        <h3>Research</h3>
		      <br>
		      <h4>How does ScienceParse work</h4>
		      <p>From the reading <a href="https://www.aclweb.org/anthology/N18-3011.pdf">Construction of the Literature Graph in Semantic Scholar</a> I have obtained information 
		      about a possible generic structure to give also to my work:</p>
		      <ol>
			      <li><b>Pre-processing of the data</b>: this first step consists of different passages which allow to transform the data in such a way that it is readable
			      by the software used to carry out the task;</li>
			      <li><b>Model</b>: it is organized the model which shall be passed from the following steps untill the final test;</li>
			      <li><b>Training</b>: train the software on a set of organized data proportioned with the final set;</li>
			      <li><b>Decoding</b>: decode the information obtained as output of the training phase to understand the quality of the results;</li>
			      <li><b>Final test</b>: test the software on a final smaller set of data to see if the predictions made by the spftware are reliable.</li>
		      </ol>
		      <p>From the notions reported in the article it has come out that there is in certain cases the need to make use of more than one software, each one for different
		      purposes. In the cited article, for instance the writers make use of 3 different ones in order to extract the titles, citations and authors: one to tokenize the
		      text, one for embedding the lowercase tokens and another to used the transformed data and carry out the final task (recognition of the paths).
		      From this article I have understood that I may try to look for a software with more features which allow to use less other external softwares to carry out the task.
		      </p>
		      <br>
		      <h4>Grobid</h4>
		      <p>GROBID definition:</p>
		      <ol>
			<li>GROBID is a machine learning library;</li>
			<li>it can extract, parse and re-structure raw documents such as PDF;</li>
			<li>it transforms raw information into structured XML/TEI encoded documents.</li>
			</ol>
		      <p>The following functionalities are available:</p>
			<ul>
				<li>References extraction and parsing. All the usual publication metadata are covered (including DOI, PMID, etc.).</li>
				<li>Citation contexts recognition and resolution to the full bibliographical references of the article.</li>
				<li>Parsing of names, in particular author names in references (distinct model from the header ones).</li>
				<li>Parsing of dates, ISO normalized day, month, year.</li>
				<li>Consolidation/resolution of the extracted bibliographical references using the biblio-glutton service or the CrossRef REST API.</li>
				<li>PDF coordinates for extracted information, allowing to create "augmented" interactive PDF.</li>
			</ul>
			<p>GROBID uses optionally Deep Learning models relying on the DeLFT library, a task-agnostic Deep Learning framework for sequence labelling and text classification.</p>
			<p>The advantage of Grobid from a computational point of view, is that it is available in three different programming languages Python, Java and Javascript (Node.js). 
				All these clients will take advantage of the multi-threading for scaling large set of PDF processing. </p>
			<p>A problem is that there is no certainty that it works on Windows system.</p>

		      <br>
		      <h4>EXCITE</h4>
		      <p>Excite is a project composed of different but dependent subsections:</p>
		      <ul>
				<li><a href=”https://github.com/exciteproject/Exparser” target=”_blank”>Exparser</a>: to extract and segment the PDF citations;</li>
				<li><a href=”https://github.com/exciteproject/EXmatcher” target=”_blank”>EXmatcher</a>: dedicated to the task of citation matching in EXCITE;</li>
				<li><a href=”https://github.com/exciteproject/EXpublisher” target=”_blank”>EXpublisher</a>: is dedicated to the task of converting EXCITE Data to a JSON file with OCC ontology;</li>
				<li><a href=”https://github.com/exciteproject/EXgoldstandard” target=”_blank”>EXgoldstandard</a>: created for the evaluation and training of EXmatcher and EXparser.</li>
			</ul>
			<p>The language is mainly python and it has the advantage of having in itself all the passages required to test and use the software 
				(it seems that it does not require external systems to make parts of the job like with ScienceParse).</p>
			<p><a href=”https://github.com/exciteproject/papers” target=”_blank”>Here</a> are the related bibliographic papers (which I have to read to better understand how Excite works).</p>
			
		      <h4>Retrieved Softwares</h4>
		      <ul>
			      <li>Parsers</li>
					<ul>
						<li><a href="https://github.com/allenai/allennlp" target=”_blank”><b>ScienceParse</b></a> is able to parse scientific papers and return them in structured form;</li>
						<li><a href="https://github.com/kermitt2/grobid" target=”_blank”><b>Grobid</b></a> for extracting, parsing and re-structuring raw documents into structured XML/TEI encoded documents;</li>
						<li><a href="https://github.com/exciteproject/" target=”_blank”><b>EXparser (EXCITE)</b></a> is a tool for extracting and segmenting reference strings from PDF documents. It is provided by the project EXCITE.</li>
					</ul>
			
			      <li>Others</li>
					<ul>
						<li><a href="https://pdfbox.apache.org/" target=”_blank”><b>Apache’s PDFBox</b></a> to tokenize the text of each single PDF page;</li>
						<li><a href="https://nlp.stanford.edu/projects/glove/" target=”_blank”><b>GloVe</b></a> to embed the lowercase tokens.</li>
					</ul>
			</ul>
		    <br>  
		<h3>Methodology</h3>
		      <ol>
			<li> As first passage look for literature letting me understand which are the main steps I’ll have to take into account for working with pdf data extraction 
			     (e.g. if I have to use one or more softwares, in which passages the software(s) is/are required); </li>
			<li>Use the scientific paper citations to look for correlated paper, only in a first phase in order to begin to understand the constraints of the research I am making;</li>
			<li>look for some relevant parsers (and eventually other softwares required in order to carry out correlated tasks) and in a second moment look to each single system and find 
				out the most relevant features and compare them. If I am not satisfied by the ones I have retrieved up to that moment I continue looking for other softwares.</li>
			<li>parameters to classify the softwares:</li>
			        <ul>
				<li>programming language (up to now java e python);</li>
				<li>number of features and need for other softwares;</li>
				<li>operating systems for which it is available (?);</li>
				<li>other (?).</li>
				</ul>
			</ol>
		  <br>    
		<h3>Bibliography</h3>
		      <ul>
			      <li><a href="https://www.aclweb.org/anthology/N18-3011.pdf" target=”_blank”>Construction of the Literature Graph in Semantic Scholar</a></li>
			    <li><a href="https://arxiv.org/abs/1802.08301">Content-Based Citation Recommendation</a></li>
			     <li><a href="https://www.aaai.org/Papers/Workshops/2007/WS-07-14/WS07-14-006.pdf" target=”_blank”>Author Disambiguation Using Error-driven Machine Learning with a Ranking Loss Function</a></li>
			     <li><a href="https://ieeexplore.ieee.org/document/7991559" target=”_blank”> Luca Weihs and Oren Etzioni. 2017. Learning to predict citation-based impact measures. In JCDL.</a></li>
			      <li><a href="https://aaai.org/ocs/index.php/WS/AAAIW15/paper/view/10185/10244" target=”_blank”>Marco Valenzuela, Vu Ha, and Oren Etzioni. 2015. Identifying meaningful citations. In AAAI Workshop (Scholarly Big Data).</a></li>
		     <li><a href="http://slides.com/janagelavik/grobid#/3" target=”_blank”>GROBID in 30 slides - GROBID Documentation</a></li>
			      <li><a href="https://grobid.readthedocs.io/en/latest/grobid-04-2015.pdf" target=”_blank”>Grobid, Metadata extraction from PDF documents using machine learning tools in INSPIRE-HEP</a></li>
			</ul>
	      </div>
	    </div>
	  </div>
<!-- end of last card -->
		
	</div> 
<!-- fine div "accordion"-->
	
	

<div class="d-flex align-items-center p-4 neutral-1-bg-a8">
  <a href="#" aria-hidden="true" data-attribute="back-to-top" class="back-to-top dark">
    <svg class="bi bi-arrow-up-square" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M8 15a.5.5 0 0 0 .5-.5V2.707l3.146 3.147a.5.5 0 0 0 .708-.708l-4-4a.5.5 0 0 0-.708 0l-4 4a.5.5 0 1 0 .708.708L7.5 2.707V14.5a.5.5 0 0 0 .5.5z"/></svg>
  </a>
</div>

</body>
</html>
